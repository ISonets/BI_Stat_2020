---
title: "Project_2"
date: "11/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Приветствую!
Нам было бы очень интересно посмотреть на то, как средняя стоимость домов (переменная medv, измерена в 1000 долларов), занимаемых владельцами, зависит от различных факторов. Это и будет нашей целью.нам нужно это для того, что заказчик исследования спрашивает, как же должен выглядеть идеальный район для постройки дома. Решением для этой задачи выступит использование линейных моделей. 

# 0. Установка и загрузка библиотек

```{r required packages installation, message=FALSE, results='hide'}
pckgs <- c('dplyr', 'ggplot2', 'car', 'psych', 'corrplot', 'MASS', 'gridExtra')
for(i in pckgs){
  if(!require(i, character.only = T)){
    install.packages(i, dependencies = T)
    library(i)
  }
}
```

```{r loading libraries if already intalled, message=FALSE, results='hide'}
library(dplyr)
library(ggplot2)  
library(car)  
library(psych)  
library(corrplot)  
library(MASS)
library(gridExtra)
```

# 1. Строим линейную модель

Загружаем данные:

```{r}
data <- Boston
str(data)
```
```{r}
summary(data)
```

Итого имеем 506 наблюдений и 14 переменных:
**Переменные:**
*crim*
-число преступлений на душу населения
*zn*
-доля земли под жилую застройку, зонированная под участки площадью более 25 000 кв. футов.
*indus*
-доля акров, не относящихся к розничной торговле, на город.
*chas*
-Переменная-индикатор(1, если дорога пересекает реку Чарльз, 0, если не пересекает)
*nox*
-концентрация оксидов азота(частицы/10 млн)
*rm*
-среднее количество комнат в доме
*age*
-доля жилых населяемых домов, построенных до 1940 г.
*dis*
-средневзвешенное расстояние до пяти бостонских центров занятости.
*rad*
-индекс доступности радиальных магистралей
*tax*
-полная ставка налога на имущество за каждые 10 000 долларов.
*ptratio*
-отношение "ученик/учитель" в городе
*black*
-1000(Bk - 0.63)^2, где Bk -- доля чернокожих в городе
*lstat*
-% населения никзого статуса
*medv*
-медианная стоимость домов(*1000$)

Проверим на NA:

```{r}
NAs <- length(which(is.na(data)==T))
if (NAs>0) {data <- data[complete.cases(data),]
}
```

NA отсутствуют, но подстраховаться от ошибок -- хорошая практика.
Сделаем *chas* факторной переменной:

```{r}
data$chas <- factor(data$chas)
data$rad <- factor(data$rad)
```


Из summary видно, что *crim*, *zn*, *rm* и *black* имеют большую разницу между медианой и средним значением,  что может указывать на наличие выбросов. Давайте проверим это.

```{r}
boxplot(data$crim, data$zn, main = 'Outliers detection', col=c('red','blue'),names = c('crim', 'zn'))
boxplot(data$rm, main='Outliers detection for *rm*', col = 'green', names = c('rm'))
boxplot(data$black, main='Outliers detection for *black*', col = 'orange')
```

Да, выбросов действительно хватает. Что с ним можно сделать?
На данный момент я бы предпочел не трогать имеющиеся данные. Иначе лин.модель будет предсказывать для *новых* значений, а не для наших данных. И выводы могут получиться соврешенно другими. В принципе, можно будет выкинуть выбросы целиком, но пока что не будем этого делать.

Для применения линейной модели нам также важно знать об отсутствии корреляции между переменными. это важно для дальнейшей проверки модели на мультиколлинеарность модели.

В наших данных есть 2 факторные переменные. Для правильного построения матрицы корреляций необходимо их исключить.

```{r}
drops <- c('chas','rad')
corr_matrix <- cor(x = data[, !names(data) %in% drops], y = NULL, method = c("spearman"))
corrplot(corr_matrix)
```



Какие выводы можно сделать?
1. Крайне сильная корреляция (0,9)- между *rad* и *tax*: чем больше доступно магистралей, тем выше ставка.
2. Сильная отрицательная корреляция между *dis* и *indus*, *nox*, *age*.Т.е. эти районы старые, там мало жилья и плохой воздух, и до центров занятости оттуда далеко.Не лучшее место.
3. Сильная отр. корреляция между *lstat* и *medv*, что логично -- небогатое население живет в дешевом жилье.
4. Сильная положительная корреляция между *indus* и *nox*( индустриальные кварталы сильнее загрязняют воздух)
5. Сильная положительная корреляция между *indus* и *tax*(промышленность платит более высокие налоги)
6. Сильная положительная корреляция между *nox* и *age*(в старых районах воздух загрязнен сильнее)
7. Сильная положительная корреляция между *rm* и *medv*(больше комнат -- дороже жилье)
Уже сейчас можно делать некоторые выводы о выборе идеального дома, но лучше подкрепить это расчетами.
Однако стоит учесть то, что предикторы мультиколлинеарны, что может осложнить нам задачу в будущем.

Вас(заказчика) особенно интересует стоимость домов. Тогда построим линейную 
модель для предсказания стоимости домов в зависимости от различных факторов района вокруг дома.

```{r}
mod_1 <- lm(medv ~ ., data = data)
summary(mod_1)
```


Стандартизуем предикторы, чтобы найти самый влиятельный предиктор(также это необходимо потому, что разные переменнные имеют разные единицы измерений, и сравнивать напрямую некорректно):


```{r}
mod_1_scaled <- lm(medv ~ ., data = as.data.frame(sapply(data[,!names(data) %in% drops], scale)))
summary(mod_1_scaled)
```

После стандартизации коэффициенты измерены в стандартных отклонениях и их можно корректно сравнивать и обрабатывать.

Модель объясняет примерно 74% изменчивости (adj. R-squared = 0.7163). F-статистика = 116.3

Запишем модель:

**medv = -0.101 * crim + 0.118 * zn + 0.0153 * indus + 0.0742 * chas - 0.2238 * nox + 0.291 * rm + 0.00212 * age - 0.3378 * dis + 0.2897 * rad -0.0224 * ptratio + 0.09243 * black - 0.4074 * lstat**

Модель не идеальна, но ее оптимизацию мы проведем позже.

# 2. Диагностика модели

Условия применимости линейных моделей:
* линейность связи (=отсутствие паттерна в остатках)             
* отсутствие влиятельных наблюдений 
* независимость наблюдений
* нормальное распределение
* постоянство дисперсии (=отсутствие гетероскедастичности)
* отсутствие коллинеарности предикторов 

Воспользуемся *ggplot2* и функцией *fortify()*

```{r}
mod_diag <- fortify(mod_1)
head(mod_diag)
```
## 2.1. График расстояний Кука

```{r}
ggplot(mod_diag, aes(x = 1:nrow(mod_diag), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red")
```


Ни одно значение не превышает условного порога в 2 единицы. Влиятельных наблюдений нет.

## 2.2. График остатков модели от предсказанных значений

```{r}
gg_resid <- ggplot(data = mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)+
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  geom_smooth(method = 'lm')
gg_resid
```


Нелинейности не обнаружено. Но не все наблюдения находятся в зоне +/- 2 стандартных отклонения. Вероятно, что это происходит из-за наличия выбросов в изначальных данных. Не обнаружено гетероскедастичности (паттерн в форме воронки).

## 2.3. Проверка на нормальность распеределения

```{r}
qqPlot(mod_diag$.fitted)
```

Распределение более или менее соответствует нормальному. Думаю, в нашем случае некоторое отличие от идеала не критично.

## 2.4. Проверка модели на коллинеарность предикторов

Используем метод VIF. Если предиктор имеет значение VIF выше 2, то его следует исключать из модели.

```{r}
vif(mod_1)
mod_2 <- update(mod_1, .~. - tax)
vif(mod_2)
mod_3 <- update(mod_2, .~. - nox)
vif(mod_3)
summary(mod_3)
```
Итого мы имеем модель **medv = 21.26 - 0.1 * crim + 0.05 * zn + 2.63 * chas1 + 3.82 * rm - 0.017 * age - 1.31 * dis + 2.84 * rad2 + 5.3 * rad3 + 2.79 * rad4 + 2.44 * rad5 + 0.78 * rad6 + 4.78 * rad7 +  4.85 * rad8 + 3.91 * rad24 - 0.84 * ptratio + 0.01 * black - 0.55 * lstat**.
может быть, это не очень правильная запись модели, зато в ней отражен вклда каждой из градаций факторной перемнной *rad*. При некоторой сноровке и умении это облегчит понимание вклада каждой из градации факторов.

В модели не осталось мультиколлинеарности. Модель почти не потеряла в точности (adj. R-squared = 0.7265). Не все из предикторов значимы, но их мы уберем позже.

Помимо стандартных графиков для множественных моделей необходимо строить графики от предикторов, не вошедших в модель.

```{r, message=FALSE}
res_1 <- gg_resid + aes(x = tax) 
res_2 <- gg_resid + aes(x = nox)
grid.arrange(res_1, res_2, nrow = 2)
```


После фильтрации модели практически ничего не изменилось. Вероятно, это происходит из-за того, что между предикторами есть сильная корреляция. Стоит ли вернуть предикторы в модель? Вопрос пока открытый.

Чуть позже мы попробуем откорретикровать модель.

# 3. Предсказания

На данный момент после удаления коллинеарных предикторов самый важный из оставшихся предикторов -- *rm*.
Сделаем искусственный датасет и предскажем для него значения *medv*.

С факторной переменной *chas* несколько сложнее. От нее нельзя получить средние значения. Но мы можем добавить их в датасет в той же пропорции, что и в оригинальных данных, и тем самым получить правильные значения и правильные выводы.

```{r}
table(data$chas)
```
Итого имеем 471х0 и 35х1 значений. Отношение = 93% : 7 %. Для датасета в 100 значений соответственно понадобится 93х0 и 7х1. Чтобы не было "ступеньки" в конце или начале, перемешаем значения рандомно. Для воспроизовдимости поставим *seed()*.
Как поступить с *rad*?. Предлагаю взять медиану. Среднее значение не соответсвует ни действительности, ни распределению. Медиана же имеет хоть какой-то смысл(половина наблюдений имеют такое же значение).

```{r}
set.seed(2020)
test_data <- data.frame(
  rm = seq(min(data$rm), max(data$rm), length.out = 100),
  crim = mean(data$crim),
  zn = mean(data$zn),
  chas = as.factor(sample(c(rep(0, 93),rep(1,7)))) ,
  age = mean(data$age),
  rad = as.factor(median(as.numeric(data$rad))),
  indus = mean(data$indus),
  lstat = mean(data$lstat),
  dis = mean(data$dis),
  ptratio = mean(data$ptratio),
  black = mean(data$black))

pred <- predict(mod_3, newdata = test_data,  interval = 'confidence')
test_data <- data.frame(test_data, pred)

# График предсказаний модели
Pl_predict <- ggplot(test_data, aes(x = rm, y = fit)) +
  geom_ribbon(alpha = 0.2, aes(ymin = lwr, ymax = upr)) +
  geom_line() + 
  ggtitle("Множественная модель для предсказания medv")
Pl_predict 
```

Итак, предсказание построено. К сожеланию, факторная переменная *chas* портит красивый внешний вид графика. Но выкинуть ее мы не имеем права.

На этом заканчивается обязательная часть работы.


# 4. Дополнительная часть

## 4.1. Пошаговый отбор предикторов по значимости

Меня не оставляет мысль попробовать улучшить нашу модель. 63% точности -- далеко не идеальный результат.
Попробуем отобрать самые значимые предикторы из тех, что остались после удаления коллинераных предикторов.
Есть несколько алгоритмов отбора предикторов. Мы будем использовать **backward selection**. 

В качестве критерия отбора будем использовать **частный F-test**. В R удобнее пользоваться функцией `drop1()`. Она сравнивает полную модель с моделью, из которой удален предиктор. И делает это последовательно для всех предикторов в модели. Если удаление предиктора незначимо влияет на количество объясненной с помощью модели изменчивости, то такой предиктор можно удалять. 

```{r}
drop1(mod_3, test = "F")

mod_4 <- update(mod_3, .~. - age)
drop1(mod_4, test = "F")
```
После *VIF selection* и *backward selection* мы потеряли 1 предиктор(*age*).Надеюсь, мы не слишком потеряем в точности.
```{r}
summary(mod_4)
```
Радует, что модель не потеряла в точности.

## 4.2. Диагностика модели

Для множественной модели необходима такая же диагностика, что и для простой. 

Мы видим, что график остатков выглядит отлично (кроме одного наблюдения за пределами 2 стандартных отклонений), как и график расстояний Кука. Квантильный график выглядит уже хуже...но не критично.

```{r}
mod_4_diag <- data.frame(fortify(mod_4))

gg_resid_1 <- ggplot(data = mod_4_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")
gg_resid_1
```


```{r}
ggplot(mod_4_diag, aes(x = 1:nrow(mod_4_diag), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red")
```



```{r}
qqPlot(mod_4_diag$.stdresid)
```
И график остатков для удаленного предиктора:

```{r}
res_3 <- gg_resid + aes(x = age)
plot(res_3)
```

По результатам коррекции мы видим неприемлимые для нас вещи...

1. Распределение перестало быть нормальным. Так что лин. модели использовать в таком случае нельзя. Попробуем прологарифмировать. Мб условия применимости вернутся в норму.
2. Волнует еще то, что несмотря на отбор, точность остается достаточно низкой. Попробуем вернуть в модель некоторые из важных предикторов, даже несмотря на их VIF > 2.


```{r}
qplot(x=log(medv),data=data,geom='histogram')
```

Уже лучше Хоть и есть  некоторое смещение вправо, предлагаю в дальнейшем сторить лин. модель для логарифимрованных данных.

Тогда обновим модель:

```{r}
mod_4_log <- lm(formula = log(medv) ~ crim + zn + indus + chas + rm + dis + rad + 
    ptratio + black + lstat, data = data)
```

## 4.3. Коррекция оптимальной модели

### 4.3.1. Пошаговый отбор предикторов по значимости

Включим в модель обратно те предикторы, от которых остатки зависят наиболее явно. И проведем повторный backward selection нашей модели.

```{r}
mod_5<-update(mod_4_log, .~. + age + tax + nox)
drop1(mod_5, test = "F")
mod_6<-update(mod_5, .~. - indus)
drop1(mod_6, test = "F")
mod_7<-update(mod_6, .~. - age)
drop1(mod_7, test = "F")
```

```{r}
summary(mod_7)
```

Отлично. 78% точности!(adj. R-squared = 0.7844)(хоть и с учетом автокореляции). F- статистика 104.3.

Теперь наша модель имеет такой состав:

**log(medv) = 3.99 - 0.01 * crim + 0.1048 * chas + 0.09 * rm - 0.037 * ptratio + 0.0004 * black - 0.0288 * lstat - 0.711 * nox - 0.0005 * tax + 0.093 * rad2 + 0.174 * rad3 + 0.103 * rad4 + 0.132 * rad5 + 0.9 * rad6 + 0.206 * rad7 + 0.189 * rad8 + 0.33 * rad24 - 0.054 * dis + 0.0013 * zn **


### 4.3.2. Диагностика модели

После коррекции модели обязательно необходимо провести повторную диагностику

```{r}
mod_7_diag <- data.frame(fortify(mod_7))

ggplot(mod_7_diag, aes(x = 1:nrow(mod_7_diag), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red")
```


```{r}
gg_resid_2 <- ggplot(data = mod_7_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")

gg_resid_2
```



```{r}
res_4 <- gg_resid + aes(x = indus)
res_5 <- gg_resid + aes(x = age)

grid.arrange(res_4, res_5, nrow = 2)
```


```{r}
qqPlot(mod_7)
```



Все еще далеко от идеала. Особенно беспокоит распределение.

### 4.3.3. Удаление выбросов и повышение точности

Раньше я писал о том, что выбросы могут давать такие результаты. Давайте попробуем от них избавиться.

```{r}
outlierTest(mod_7)
```



```{r}
data_mod = data[-c(413,372, 373, 402),,drop=T]
row.names(data_mod)=1:nrow(data_mod)
```


Проверим результаты:


```{r}
mod_8 <- lm(formula = log(medv) ~ crim + zn + chas + rm + dis + rad + 
    ptratio + black + lstat + tax + nox, data = data_mod)
summary(mod_8)
```

Выиграли еще почти 3% точности(adj.R-squared =0.8117).


Итого формула на данный момент:
**log(medv) = 3.773 - 0.01 * crim + 0.001 * zn + 0.08 * chas + 0.1 * rm - 0.036 * ptratio + 0.0005 * black - 0.028 * lstat - 0.60 * nox - 0.0005 * tax + 0.09 * rad2 + 0.176 * rad3 + 0.106 * rad4 + 0.132 * rad5 + 0.09 * rad6 + 0.2 * rad7 + 0.187 * rad8 + 0.32 * rad24 - 0.049 * dis**


Проведем диагностику модели:


```{r}
mod_8_diag <- data.frame(fortify(mod_8))

ggplot(mod_8_diag, aes(x = 1:nrow(mod_8_diag), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red")
```




```{r}
gg_resid_3 <- ggplot(data = mod_8_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")

gg_resid_3
```


```{r}
qqPlot(mod_8)
```


QQ-plot все еще далек от идеала, но лучше, чем было до этого. Попробуем еще поискать выбросы?


```{r}
outlierTest(mod_8)
```


```{r}
data_mod2 <- data_mod[-c(369),,drop=T]
row.names(data_mod2)=1:nrow(data_mod2)
mod_9 <- lm(formula = log(medv) ~ crim + chas + rm + ptratio + black + 
    lstat + nox + tax + rad + dis, data = data_mod2)
summary(mod_9)
```
Добавили еще немного мощности. Но прирост краней незначителен(adj.R-su=quared = 0.816).

Для очистки совести давайте посмотрим на выбросы еще раз:

```{r}
outlierTest(mod_9)
```
```{r}
data_mod3 <- data_mod2[-c(372,398),,drop=T]
row.names(data_mod3)=1:nrow(data_mod3)
mod_10 <- lm(formula = log(medv) ~ crim + chas + rm + ptratio + black + 
    lstat + nox + tax + rad + dis, data = data_mod3)
summary(mod_10)
```
Adj.R-squared = 0.823. Кажется,мы достигли потолка нашей модели. Впрочем, прирост в 10% точности вполне оправдан. На данный момент это лучшее, что у меня получалось. Такая точность более чем достаточна для надежных предсказаний.

## 5. Предсказание с использованием улучшенной и оптимизированной модели.

Сгенерируем новый датасет:

```{r}
set.seed(2021)
test_data_2 <- data.frame(
  rm = seq(min(data$rm), max(data$rm), length.out = 100),
  crim = mean(data$crim),
  zn = mean(data$zn),
  chas = as.factor(sample(c(rep(0, 93),rep(1,7)))) ,
  age = mean(data$age),
  rad = as.factor(median(as.numeric(data$rad))),
  indus = mean(data$indus),
  nox = mean(data$nox),
  lstat = mean(data$lstat),
  dis = mean(data$dis),
  ptratio = mean(data$ptratio),
  tax = mean(data$tax),
  black = mean(data$black))

pred_2 <- predict(mod_10, newdata = test_data_2,  interval = 'confidence')
test_data_2 <- data.frame(test_data_2, pred_2)

# График предсказаний модели
Pl_predict_2 <- ggplot(test_data_2, aes(x = rm, y = fit)) +
  geom_ribbon(alpha = 0.2, aes(ymin = lwr, ymax = upr)) +
  geom_line() + 
  ggtitle("Множественная модель для предсказания medv после улучшений")
Pl_predict_2 
```

## 6. Заключение

Итак, если отрешиться от цифр и хода анализа данных, что можно посоветовать заказчику для того, чтобы продать идеальный дом?
Во-первых, обратиться к нам:)
Во-вторых, внимательно ознакомиться с выкладками. Невнимательный заказчик увидит только график, и начнет искать дома с максимально большим количеством комнат. Математически он будет прав, но его решение будет неправильным. Дьявол, как обычно, в деталях.
**Всегда** стоит более подробно взглянуть и на другие предикторы, которые может быть не столь важны для анализа, но важны для жизни.
Стоит искать дом с максимально возможным числом комнат, по возможности в районах с низким уровнем преступности, с видом на реку Чарльз, в возможно близкой доступности магистралей и не очень далеко от центров занятости. 
Немаловажным будет и отношение ученик/учитель, это важно для удобства проживания семейных пар; чем выше этот параметр, тем больше школ поблизости. 
Чтобы продать дом дороже, желательно сторить его в кварталах, в которых живут представители среднего класса и более обеспеченные люди.
Очень желательно строить дом в экологически благоприятных местах, т.к. концентрация оксидов азота немаловажный параметр. Избыток воздушного загрязнения негативно скажется на продажах и последующих жильцах. 
Налоги и черное население поблизости не оказывают существенного влияния на модель и дальнейшие приготовления(стройка и продажа).
Как видите, при небольших усилиях можно получить гораздо более объемную и понятную информацию, что позволит в дальнейшем грамотно выстроить стратегию.
Спасибо за внимание!



